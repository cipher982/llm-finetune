{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/data/hf/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig \n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0326c9ce10d454fb46bfb911be73363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a140e20086461d9bcfaa4d7e98630e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e91246f6e2459fb37a19f22c553f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5e5494a90a4c6ca80150792944e552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"cipher982/report_builder\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cipher982/report_builder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><START_Q> How many clicks did I have last 2 weeks?<END_Q><START_A>. nobody knows.\n",
      "How many clicks did I have last 2 weeks?\n",
      "I have no idea. I don't know how many clicks I had last week. I don't know how many clicks I had last month. I don't know how many clicks I had last year. I don't know how many clicks I had last decade. I don't know how many clicks I had last century. I don't know how many clicks I had last millennium. I don't know how many clicks I had last eon. I don't know how many clicks I had last epoch. I don't know how many clicks I had last geological era. I don't know how many clicks I had last glacial period. I don't know how many clicks I had last interglacial period. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice age. I don't know how many clicks I had last ice\n"
     ]
    }
   ],
   "source": [
    "input_text = \"How many clicks did I have last 2 weeks?\"\n",
    "input_text =  \"<START_Q>\" + input_text + \"<END_Q>\" + \"<START_A>\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=500, \n",
    "        eos_token_id=32003, \n",
    "        temperature=.1,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # return_dict=True\n",
    "    )\n",
    "\n",
    "# Decode the generated output back to text\n",
    "generated_text = tokenizer.decode(output[0][0], skip_special_tokens=False)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the scores for the EOS token at each generation step\n",
    "eos_scores = [score[0][2].item() for score in output.scores]\n",
    "\n",
    "# Get the top-10 token scores and their corresponding token IDs at each generation step\n",
    "top_k = 10\n",
    "top_scores = []\n",
    "for score in output.scores:\n",
    "    values, indices = score[0].topk(top_k)\n",
    "    top_scores.append(values.tolist())\n",
    "\n",
    "# Transpose the list for plotting\n",
    "top_scores_transposed = list(map(list, zip(*top_scores)))\n",
    "\n",
    "# Plot the scores\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(top_k):\n",
    "    plt.plot(top_scores_transposed[i], label=f\"Top {i+1}\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(eos_scores, label=\"EOS\", color=\"black\", linewidth=2)\n",
    "plt.xlabel(\"Generation Step\")\n",
    "plt.ylabel(\"Score (logit)\")\n",
    "plt.title(\"Comparison of EOS Score with Top-10 Token Scores\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the scores for the EOS token at each generation step\n",
    "eos_scores = [score[0][2].item() for score in output.scores]\n",
    "\n",
    "# Get the top-10 token scores and their corresponding token IDs at each generation step\n",
    "top_k = 10\n",
    "top_scores = []\n",
    "top_tokens = []  # to store the actual token strings\n",
    "\n",
    "for score in output.scores:\n",
    "    values, indices = score[0].topk(top_k)\n",
    "    top_scores.append(values.tolist())\n",
    "    # Convert token IDs to strings and store them\n",
    "    tokens = [tokenizer.decode([idx.item()]) for idx in indices]\n",
    "    top_tokens.append(tokens)\n",
    "\n",
    "# Transpose the list for plotting\n",
    "top_scores_transposed = list(map(list, zip(*top_scores)))\n",
    "\n",
    "# Plot the scores\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(top_k):\n",
    "    label_token = top_tokens[0][i]  # You can use tokens from any generation step. Here we use the first step as an example.\n",
    "    plt.plot(top_scores_transposed[i], label=f\"{label_token}\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(eos_scores, label=\"EOS\", color=\"black\", linewidth=2)\n",
    "plt.xlabel(\"Generation Step\")\n",
    "plt.ylabel(\"Score (logit)\")\n",
    "plt.title(\"Comparison of EOS Score with Top-10 Token Scores\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the scores for the EOS token at each generation step\n",
    "eos_scores = [score[0][2].item() for score in output.scores]\n",
    "\n",
    "# Get the top token score at each generation step\n",
    "top_scores = [score[0].max().item() for score in output.scores]\n",
    "\n",
    "# Plot the scores\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(eos_scores, label=\"EOS\", color=\"black\", linewidth=2)\n",
    "plt.plot(top_scores, label=\"Top Score\", color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Generation Step\")\n",
    "plt.ylabel(\"Score (logit)\")\n",
    "plt.title(\"Comparison of EOS Score with Top Score at Each Generation Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolly2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
